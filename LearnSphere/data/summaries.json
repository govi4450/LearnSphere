{
    "ukzFI9rgwfU": " machines learn from their past experiences and machines follow instructions given by humans but what if humans can train the machines to learn from the past data and do what humans can do and much faster well that's called machine learning. machine learning is more than just learning it's also about understanding and reasoning so today we will learn about the basics of machine learning so that's paul he loves listening to new songs he either likes them or dislikes them paul decides this on the basis of the song's tempo genre intensity and the gender of voice.  supervised learning uses labeled data to train the model here the machine knew the features of the object and also the labels associated with those features on this note. Unsupervised learning uses unlabeled data and then there is reinforcement learning which is a reward based learning or we can say that it works on the principle of feedback here. We provide feedback to the training model and ask it to predict until it learns i hope you've understood supervised and unsupervisedlearning so let's have a quick quiz.  machine learning is used in healthcare where diagnostics are predicted for doctor's review the sentiment analysis that the tech giants are doing on social media is another interesting application of machine learning fraud detection in the finance sector and also to predict customer churn in the e-commerce sector. While booking a gap you must have encountered surge pricing often where it says the fair of your trip has been updated continue booking.",
    "9gGnTQTYNaE": "Luv Aggarwal is a Data Platform Solution engineer for IBM. He explains what machine learning, or ML, is and how to use it in your business. He also explains the different types of machine learning and how they can be used in different use-cases, such as customer retention. He concludes by sharing his thoughts on the future of AI and machine learning in the digital world. Unsupervised learning is when we use machine learning algorithms to analyze and cluster unlabeled data sets. This method helps us discover hidden patterns or groupings without the need for human intervention. The last type of machine learning I want to talk about today is called \u2018reinforcement learning\u2019 This is a form of semi-supervisedlearning where we typically have an agent or system take actions in an environment. I encourage you to dive deeper and learn more about it. If you want to know what are some of the common machine learning algorithms and how to leverage them in data science, please check out the links in the description. And don't forget, you can grow your skills and earn a badge with IBM Cloud Labs, which are free browser-based interactive Kubernetes labs. Thank you.",
    "Fa_V9fP2tpU": " machine learning is a branch of artificial intelligence that enables computers to learn from data and improve their performance on tasks over time. here a list of all basic machine learning terms in 22 minutes. artificial intelligence refers to the capability of machines to perform tasks that typically require human intelligence. This can include understanding language recognizing images solving problems or making decisions AI aims to mimic human cognitive functions through various techniques including machine learning but not all AI is machine learning. A model in machine learning is a mathematical representation that is trained to recognize patterns in data and make predictions or classifications based on those patterns. The most common type of model is simply a mapping function between a an input and an output in linear regression. Training or learning is the process of adjusting a model's parameters to find the best match between the model's predictions and the actual data. The test and training data are separated randomly before beginning the modeling process. Unsupervised learning is a type of machine learning where models learn to find patterns and structure in data without being given labeled examples or correct answers. Reinforcement learning is the newer branch of machinelearning that has recently been accepted as a third main branch. It operates on a fundamentally different principle instead of learning from pre-labeled examples instead of finding patterns in unlabeled data unsupervised it learns from interaction and feedback.  feature engineering involves using domain knowledge and creativity to transform or combine original features into more meaningful ones. Good feature engineering often makes the difference between an average model and an excellent one as it helps the model focus on the most relevant patterns in the data feature scaling also called normalization or standardization is the process of transforming numeric features to a similar scale typically to prevent features with larger ranges from dominating the learning process. dimensionality reduction techniques are often crucial in machine learning helping to compress many features into a smaller set while preserving important information. Model complexity refers to how sophisticated a machine learning model is in terms of its ability to capture patterns in the data. A more complex model has more parameters and can learn more complicated relationships like a neural network with many layers. Finding the right level of complexity is crucial too simple and the model fails to capture important patterns which is called underfitting too complex and it learns to fit to noise in the training data rather than true patterns also called overfitting. The bias variance trade-off is a fundamental Concept in machine learning that describes the tension between a model's ability to minimize bias and variance. High variance often indicates overfitting where the model is learning the random noise and the training data rather than the true underlying patterns. The Sweet Spot in this tradeoff is crucial the goal is to create a model that's complex enough to capture true patterns in the data but not so complex that it fits to noise. The model is trained five times each time using a different part as the validation set and the remaining parts for training. validation sets are used during the model development process to make decisions about hyperparameters and model selection the test set is kept completely separate and used only once at the very end to evaluate the final model's performance. Regularization refers to techniques used to prevent overfitting by adding constraints or penalties that discourage a model from becoming too complex or fitting too closely to the train training data.  gradient descent is a fundamental optimization algorithm used to train machine learning models by iteratively adjusting model parameters to minimize errors. gradient descent calculates the direction in which the model's error decreases most rapidly and updates the parameters accordingly for each step. The learning rate is a crucial hyperparameter that determines how much a model adjusts its parameters in response to errors during training like a student adjusting their understanding based on on feedback. Finding the right learning rate is often critical for successful training too high and the model might never converge too low and training might take unnecessarily long evaluation is the process of measuring how well a machine learning model performs on data it hasn't seen during training. this process typically involves both validation to tune the model during development and testing. evaluation helps determine whether a model has truly learned use patterns or has just memorized the training data.",
    "E0Hmnixke2g": "In 17 minutes Tim will give you an overview of the most important machine learning algorithms to help you decide which one is right for your problem. Tim has been a data scientist for over 10 years and taught all of these algorithms to hundreds of students in real life machine learning boot camps. In 17 minutes you will know how to pick the right algorithm for any problem and get a basic intuition of each algorithm and how they relate to each other. Machine learning algorithms try to determine the relationship between two variables. In classification we try to assign a discrete categorical label also called a class to a data point. Logistic regression is a variant of linear regression and probably the most basic classification algorithm. Instead of fitting a line to two numerical variables with a presumably linear relationship you now try to predict a categorical output variable using categorical or numerical input variables. The K nearest neighbors algorithm or KNN is a very simple and intuitive algorithm that can be used for both regression and class ification. The name means that we don't try to fit any equations and thus find any parameters of a model so no true model fitting is necessary. The core concept of the algorithm is to draw a decision boundary between data points that separates data points of the training set as well as possible. SPM is very powerful in high Dimensions that is if the number of features is large compared to the size of the data in those higher dimensional cases the decision boundary is called a hyperplane. Another feature that makes SPM extremely powerful is the use of so-called kernel functions which allow for the identification of Highly complex nonlinear decision boundaries. kernel functions are an implicit way to turn your original features into new more complex features.  artificial neural networks are designed to implicitly and automatically design these features for us without any guidance from humans we do this by adding additional layers of unknown variables between the input and output variables in its simplest form this is called a single layer percep chop which is basically just a multi-feature regression task. If we add a hidden layer the hidden variables in the middle layer represent some hidden unknown features and instead of predicting the target variable directly we try to classify the image. Deep learning can result in very complex hidden features that might represent all kinds of complex information in the pictures. We don't usually know what the hidden features represent we just train the neural network to predict the final Target as well as possible. The most famous clustering algorithm is called K means clustering just like for KNN K is a hyperparameter and stands for the number of clusters you are looking for. K means is the most famous and most common clustering algorithm. dimensionality reduction is to reduce the number of features or dimensions of your data set keeping as much information as possible. This group of algorithms does this by finding correlations between existing features and removing potentially redundant Dimensions without losing much information. If you are overwhelmed and don't know which algorithm you need here is a great cheat sheet by syit learn to help you decide which algorithm is right for which type of problem.",
    "6M5VXKLf4D4": "Deep learning is a type of machine learning inspired by the structure of the human brain. It is a subset of artificial intelligence which is a technique that enables a machine to mimic human behavior. The features are picked out by the neural network without human intervention. Where is deep learning applied in customer support when most people converse with customer support agents the conversation seems so real they don't even realize that it's actually a bot. Deep learning is the most efficient way to deal with unstructured data a neural network requires a massive volume of data to train. neural networks detect cancer cells and analyze mri images to give detailed results self-driving cars what seem like science fiction is now a reality. horus technology working on a device for the blind that uses deep learning with computer vision to describe the world to the users replicating the human mind at the entirety.",
    "i_LwzRVP7bg": "Kylie Ying is a physicist, engineer, and basically a genius. She's going to teach you about machine learning in a way that is accessible to absolute beginners. In this video, we'll talk about supervised and unsupervised learning models. We'll go through maybe a little bit of the logic or math behind them, and then we'll also see how we can program it on Google CoLab. Pandas reads a CSV file that you pass in and turns that into a pandas data frame object. If we call the head is just like, give me the first five things, give us the first 5 things. Now you'll see that we have labels for all of these. Okay, great. Now in order to label those as these columns down here in our data frame. So what we're going to do is we'll convert this to zero for G and one for H. Machine learning is a sub domain of computer science that focuses on certain algorithms. It might help a computer learn from data, without a programmer being there telling the computer exactly what to do. There are a few types of machine learning. In supervised learning, we're using labeled inputs. And data science is a field that attempts to find patterns and draw insights from data. And that might mean we\u2019re using machine learning or AI. In this class, we'll be focusing on supervised learning and unsupervised learning and learning different models for each of those. In supervised learning, all of these inputs have a label associated with them, this is the output that we might want the computer to be able to predict. Now there's also un Supervised learning, we use unlabeled data to learn about patterns in the data. And finally, we have reinforcement learning, they usually there's an agent that is learning in some sort of interactive environment. There are two types of categorical data: quantitative and qualitative. quantitative data are numerical valued pieces of data. qualitative data is more like a rating system, like good is closer to great than it is to really bad. In both cases, there's no inherent order. It's not like, you know, we can rate us one and France to Japan three, etc. Right? There's not really any inherent order built into either of these categoricalData sets. In supervised learning, there are some different tasks, there's one classification, and basically classification, just saying, okay, predict discrete classes. This is something known as multi class classification. But there's also binary classification. And binary classification, you might have hot dog, or not hot dog. So there's only two categories that you're working with. We're trying to predict a number that's as close to the true value as possible using different features of our data set. Each row is a different sample in the data. data set. And here we have different number of pregnancies, different glucose levels, blood pressure, skin thickness, insulin, BMI, age, and then the outcome whether or not they have diabetes. And all of these together is our features matrix x. And over here, this is our labels or targets vector y. So I've condensed this to a chocolate bar to kind of talk about some of the other concepts in machine learning. We break up our whole data set that we have into three different types of data sets, we call it the training data set, the validation data set and the testing data set. So what we do is then we feed the trainingData set into our model, we come up with a vector of predictions corresponding with each sample that we put in our model. And then we figure out, okay, what's the difference between our prediction and the true values, this is something known as loss, losses, and then we make adjustments. Loss is the difference between your prediction and the actual, like, label. In computer science, we like formulas, right? We like formulaic ways of describing things. This here is known as L one loss. And this is for binary classification, this this might be the loss that we use. So this loss, you know, I'm not going to really go through it too much. But you just need to know that loss decreases as the performance gets better. Matplotlib is a free, open-source computer program that plots data. It can be used to plot data in a variety of ways, including histograms. In this example, it plots a histogram of 10 different features. The histograms are normalized so that they are distributed over how many samples there are. It also plots the length, alpha, density, title and label of each feature.  NumPy dot split, I'm just splitting up the data frame. And if I do this sample, where I'm sampling everything, this will basically shuffle my data. Now, if we go up here, and we inspect this data, we'll see that these columns seem to have values in like the 100s, whereas this one is 0.03. So the scale of all these numbers is way off. And sometimes that will affect our results. In the training data set, there are around 7000 gammas, but only around 4000 of the hadrons. So that might actually become an issue. So we want to oversample our our trainingData set. And surprise, surprise, there is something that we can do to increase the number of these values, so that these kind of match better. In order to do that, I'm going to call H stack. H stack is saying, okay, take an array, and another array and horizontally stack them. The first model that we're going to learn about is KNN or K nearest neighbors. And then I'm going to show you how we can do that in our code. So I will actually add in this parameter called oversample, and set that to false for default. And what that's doing is saying, okay, take more of the less class. So take take the lessclass and keep sampling from there. The plot is based on the Euclidean distance. The distance is equal to the square root of one point x minus the other points x squared plus extend that square root, the same thing for y. So let's say that somebody makes 40,000 a year and has two kids. What do we think that would be? Well, just logically looking at this plot, you might think, okay, it seems like they wouldn't have a car, right? In the nearest neighbor algorithm, we see that there is a K, right? And this K is basically telling us, okay, how many neighbors do we use in order to judge what the label is? So usually, we use a K of maybe, you know, three or five, depends on how big our data set is. But here, I would say, maybe a logical number would be 3 or five. We're actually going to use a package from SK learn. So the reason why we, you know, use these packages and so that we don't have to manually code all these things ourselves, because it would be really difficult. So first, let's see what happens if we just use one. So now if I do K, and then model dot fit, I can pass in my x training set and my weight y train data. In naive Bayes, we have to understand conditional probability and Bayes rule. We have some sort of data set that's shown in this table right here. People who have COVID are over here in this red row. And people who do not have COVI are down here in the green row. Now, we're going to mostly look at this one because we have an unbalanced test data set. Bayes rule is asking, okay, what is the probability of some event A happening, given that B happened? So this, we already know has happened. Well, what if we don't have data for that, right? Well, you can actually go and calculate it, as long as you have a probability of B given A. And this is just a mathematical formula for that. And let's actually see Bayes rule in action. Let's use it on an example. A false positive is when you test positive, but you don't actually have the disease. A false negative is a probability that you test negative given that you actually have a disease. The probability of having a positive test given that I have the Disease is 0.99. And the probability of the Disease being point 0.1 just means I have 10% probability of actually having the disease, right? The Bayes rule asks, What is the probability of some class CK? So by CK, I just mean, you know, the different categories, so C for category or class or whatever. And then this down here is called the evidence because what we're trying to do is we're changing our prior, we're creating this new posterior probability built upon the prior. And that evidence is a probability of x. In naive Bayes, the point of it being naive, is that we're actually this joint probability, we're just assuming that all of these different things are all independent. So that means that I can actually write this part of the equation here as this. So each term in here, I can just multiply all of them together. So this is proportional to x one, x two, x n given class K times the probability of that class. The Bayes rule for helping us approximate that right in something that maybe we can we have like the evidence for that, we have the answers for that based on our training set. So this principle of going through each of these and finding whatever class whatever category maximizes this expression on the right, this is something known as MAP for short, or maximum a posteriori. So that is naive Bayes. Back to the notebook. Using a naive Bayes model, we get worse scores, right? Our precision, for all of them, they look slightly worse. And our total accuracy, I mean, it's still 72%. Which, you know, is not not that great. Okay, so let's move on to logistic regression. Here, I've drawn a plot, I have y. So this is my label on one axis. And then this is maybe one of my features. In probability, we know probably one of the rules of probability is that probability has to stay between zero and one. So how do we fix this? Well, maybe instead of just setting the probability equal to that, we can set the odds equal to this. So by that, I mean, okay, let's do probability divided by one minus the probability. Now this ratio is allowed to take on infinite values. We're basically trying to fit our data to the sigmoid function. And when we only have, you know, one data point, that's what we call simple logistic regression. But then if we have multiple features that we're considering when we're building our model, we call it multiple logistic regressions. So here, this is decent precision 65% recall 71, f, f. SVM is something called support vector machines, or SVMs for short. In an SVM model, I have two different features x zero and x one on the axes. My goal is to find some sort of line between these two labels that best divides the data. So in a line like this, the closest values to this line might be like here. And I'm 82 total accuracy of 77. 168, or 82 per cent. SVMs can be used to divide data sets into classes. The goal is to find the line that best separates the two different classes. These are what we call support vector machines, or SVMs. SVM can be difficult to use if there are outliers in your data set, but it's possible to use SVMs if you know how to work with them. The best way to use SVM is to start with a one dimensional data set. Sk learn.svm, we are going to import SVC. And SVC is our support vector classifier. So with our SVM model, we're going to, you know, create SVC model. And we aregoing to, again, fit this to X train, I could have just copied and pasted this, I should be able to do that. So if I make predictions, you'll note that Wow, the accuracy actually jumps to 87% with the SVM. Now, let's see if we can actually beat that using a neural net. Each of these layers in here, this is something known as a neuron. In a neural net, these are all of our features that we're inputting into the neural net. And the sum of all of these goes into that neuron. And then after applying this activation function, we get an output. And this is what a neuron would look like. Now a whole network of them would look something like this. With these activation functions, every single output of a neuron is no longer just the linear combination of these, it's some sort of altered linear state. So this is a training set, the model, the loss, right? And then we do this thing called training, where we have to feed the loss back into the model and make certain adjustments to the model to improve this predicted output. TensorFlow is an open source library that helps you develop and train your ML models. So we're using a sequential neural net. sequential is just, you know, what we've seen here, it just goes one layer to the next. And a dense layer means that all of them are interconnected. So here, this is interconnected with all of these nodes, and this one's all these, and then this one gets connected to all of the next ones. And then our output layer is going to be just one node. So we are going to use TensorFlow, and I don't think I imported that up here. So I'm going to import Tensor Flow as TF. And enter. neural net for classification. So essentially, this is saying layer all these things that I'm about to pass in. And at first we have to specify the input shape. So here we have 10, and comma. Alright, so that's our first layer. Now our next layer, I'm just going to have another dense layer of 32 units all using relu. TensorFlow is great, because it keeps track of the history of the training, which is why we can go and plot it later on. Now I'm going to set that equal to this neural net model. And fit that with x train, y train, and make the number of epochs equal to 100. And the batch size, I'mgoing to set equal to, let's say 32. And we can run this. And here we do see that, you know, our validation accuracy improves. So we're going to do x train y train, the number of nodes, the dropout, the probability that we just talked about learning rate. rewrite this so that we can do something what's known as a grid search. So we can search through an entire space of hey, what happens if, you know, we have 64 nodes and 64 nodes, or 16 nodes and 16 nodes, and so on. And then on top of all that, we can change this learning rate, and how many epochs we can changes. Let's say I want to record whatever model has the least validation loss. So I'm going to print out all of the parameters. that we've defined here, dropout, prob, LR, batch size, and epochs. And then now we have both the model and the history. And I'm also going to plot the accuracy. Probably should have done them side by side, that probably would have been easier. The accuracy is around 88%, which is pretty good. So this accuracy here is on the validation data set that we've defined at the beginning, right? And this one here, this is actually taking 20% of our tests, our training set every time during the training, and saying, Okay, how much of it do I get right now? You know, after this one step where I didn't train with any of that. So they're slightly different. The accuracy gives us 87%. So it seems like what happened here is the precision on class zero. So the hadrons has increased a bit, but the recall decreased. But the F one score is still at a good point eight one. And for the other class, it looked like the precision decreased a bit the recall increased. That's also been increased. I mean, we went through all this work and we got a model that performs actually very, very similarly to the SVM model that we had earlier. The formula for linear regression is based on the concept of something known as a residual. The residual is the distance between some given point, and its prediction, its corresponding prediction on the line. So now, with this residual, this line of best fit is generally trying to decrease these residuals as much as possible. And that might mean, you know, minimizing the sum of all the residuals. Simple linear regression is when we try to come up with the best prediction for all the data points in our data set. Now, there's also multiple linear regression, which just means that if we have more than one value for x, then our predictor might look something more like this. Now you guys might have noticed that I have some assumptions over here. And you might be asking, okay, Kylie, what in the world do these assumptions mean? So let's go over them. So suppose I have a data set, look like this. And here are all my data points, right? And now let's say my line looks something like that. So my mean absolute error would be summing up all of these values. This was a mistake. So what would be the distance? So the mathematical formula for that would be, okay, let's take all the residuals. Alright, so this is the distance. Mae is good because it allows us to, you know, when we get this value here, we can literally directly compare it to whatever units the y value is in. An evaluation technique that's also closely related to that is called the mean squared error. MSE is something along the lines of, okay, let's sum up something, right, so we're going to sum up all of our errors. The root mean squared error is the square root of a big squared root. The coefficient of determination, or r squared, is the sum of the squared residuals. TSS is the total sum of squares. The further a few data points are from the line of best fit, the larger the error. The square means that we're punishing larger errors, so even if they look somewhat close in terms of distance, then the further the error is. The total sum of squares is the sum of the squared residuals. It tells us what is our error with respect to this line of best fit. And if our errors are much smaller, then that means that this ratio of the RSS over TSS is going to be very small, which means that R squared isgoing to go towards one. And now that's linear regression. And then we've gone through all the different ways of evaluating a linear regression model and the pros and cons of each one. So again, I'm on the UCI machine learning repository. And here I found this data set about bike sharing in Seoul, South Korea. Now I'm going to import a bunch of the same things that I did earlier. And, you know, continue to import the oversampler and the standard scaler. And then I'm actually also just going to let you guys know that I have a few more things I wanted import. So let's say we're going to create this. We want it to be in zeros and ones. And we're just going to index on an hour, and we're only going to use that specific hour. We just have the by count, the temperature, humidity, wind, visibility, and yada,. yada, yada. So I'm actually going to get rid of some of these columns that, you know, I don't really care about. can get the data. Numpy dot split is a way to split a data set into training, validation and test data set. Here, we can use the exact same thing that we just did. And we're going to do that. But now set this to eight. Okay. So if I look at my data set, now, I have just the temperature, the humidity, the dew point temperature, radiation, rain, and snow. And I'm actually going to pass in what the name of the y label is, and what the x is. The temperature dimension does really well. And we might be able to use that to predict why. So if I look at x train temp, it's literally just the temperature. And then I'm going to, you know, make a linear regression model. And just like before, I can simply fix fit my x traintemp, y train temp in order to train train this linear regression models. So I'm going to just say, let's take 100 things from there. So I'mgoing to plot x, and I'm. going to take this temper, this, like, regressor, and predict x with that. Okay, and this label, I am going to label that the fit. I can get the the r squared score, so I can score x test and y test. All right, so it's an r squared of around point three eight, which is better than zero. With tensorflow, you can actually do regression, but with the neural net. So I'm not going to bother with splitting up the data again, I'm just going to go ahead and start building the model. So in this linear regression model, typically, you know, it does help if we normalize it. So that's very easy to do with tensor flow Keras layers, and get the normalization layer. And then I'm going to say, hey, just give me one single dense layer with one single unit. And that's going to be my model. So let's see what would happen if we use a neural net, a real neural net instead of just, you know, one single node in order to predict this. So here, if I call fit, I can just fit it, and I'm going to take the x train with the temperature, but reshape it. Y train for the temperature. And for the final output, I just want one cell. And this activation is also going to be Relu, because I can't ever have less than zero bytes. A neural net model is used to train a machine learning algorithm. The model is trained against a history of data, and valid validation data. After the model is done training, it has a loss rate of 0.001. The loss rate is decreasing, which is a good sign, but it's not a linear predictor, right? So actually, what if I got rid of this activation? Let's train this again. And see what happens. When we're trying to fit this model, instead of temp, we're going to use our larger data set with all the features. And of course, we want to plot the loss. Okay, so that's what our loss looks like. So an interesting curve, but it's decreasing. So before we saw that our R squared score was around point five, two. Well, we don't really have that with a neural net anymore. But one thing that we can measure is hey, what is the mean squared error, right? K means clustering is trying to do clustering using unlabeled data. In supervised learning, we have data with a bunch of features. But each of those samples has some sort of label on it, whether that's a number, a category, a class, etc. We were able to use that label in order to try to predict right, and then use that to predict new labels of other points. The computer is trying to compute k clusters from the data. If we have k equals three, for example, then okay, this seems like it could be a cluster. And if k equals two, then we might pick two different clusters. So we could have three different clusters in the data set. Now, this k here is predefined, if I can spell that correctly, by the person who's running the model. So we go back and we compute the distance between all the points and the centroids. And then we assign them to the closest centroid. So the reds are all here, it's very clear. the rest of them are probably closer to green. So let's just put all of these into green here, like that. And cool. So now we have, you know, our two, three, technically centroid, right? Unsupervised learning is trying to find some structure, some pattern in the data. And this process, something known as expectation maximization. We use this in order to compute the centroids, assign all the points to clusters. And then we're recomputing all that over again, until we reach some stable point where nothing is changing anymore. And so that's when we know that we can stop iterating between steps two and three. Principal component analysis is a way of trying to find direction in the space with the largest variance. PCA is saying, okay, map all of these points onto this one dimensional space. So the transformed data set would be here. This one's on the data sets are on the line. So we just put that there. But now this would be our new one dimensional data set. Okay, this in this direction, this is where our points are the most spread out. In principal component analysis, we're taking the difference from our current point in two dimensional space, and then it's projected point. And we're saying, alright, how much distance is there between that projection residual, and we're trying to minimize that for all of these points. So that actually equates to this largest variance dimension, this dimension here, the PCA dimension, you can either look at it as minimizing, minimize, let me get rid of this, the projection residuals. Principal component analysis is a way of looking at data that's unlabeled. We're trying to make some sort of estimation, like some guess about its structure from that original data set. PCA is saying if we have something with all these different dimensions, but we can't show all of them, how do we boil it down to just one dimension? And that is exactly either you minimize the projection residuals, or you maximize the variance. In this example, in unsupervised learning, we don't actually have access to the class. So I'm going to just try to plot these against one another and see what happens. So basically, we can see perimeter and area we give we get these three groups. The area compactness, we get this three groups, and so on. So these all kind of look honestly like somewhat similar. K means clustering. So from SK learn, I'm going to import k means. And just for the sake of being able to run, you know, any x and any y,. I'm just going to say, hey, let's use some x. What's a good one, maybe. So x could be perimeter, y could be asymmetry. Okay. And for this, the x values, I's going to just extract those specific values. With higher dimensions, we can plot the k means classes. And you'll see that, honestly, like it doesn't do too poorly. But what's actually really cool is even something like, you know, if we change. one of them where they were like on top of each other? Okay, so compactness and asymmetry, this one's messy. But in general, there are the three groups are there, right? From SK learn decomposition, I can import PCA and that will be my PCA model. So if I do PCA component, so this is how many dimensions you want to map it into. And you know, for this exercise, let's do two. Okay, so now I'm taking the top two dimensions. And my transformed x is going to be PCA dot fit transform, and the same x that I had up here. Unsupervised clustering is a type of machine learning. It's a way of teaching computers how to cluster together. This week, we'll look at how this can be applied to real-life situations. We'll also look at some of the challenges that unsupervised learning can face in the real world. And we'll end the course with a question-and-answer session.",
    "vdRp_w9f-qM": "Ivan savov's Noble guide to math and physics found this to be a great refresher to math fundamentals. Joel gruse first principles with python this is a brilliant book to understand the nuts and bolts behind some of the most popular machine learning algorithms. Quest illustrator guide to machine learning a triple band these are the books I've been devouring over the last couple of months. I also bought his guide to linear algebra data science from scratch."
}